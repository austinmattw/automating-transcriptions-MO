{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43aaadb3-0632-4ef5-bc4d-44ea76c93368",
   "metadata": {},
   "source": [
    "# Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec04d2-f2bc-44d9-bef8-0809fa1cc7e3",
   "metadata": {},
   "source": [
    "Set input_folders as the path to the main folder the images are stored within. Note that the main folder can be structured so that it contains subfolders of images. At the Missouri Botanical Garden, image file names are specimen barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95bd11-9ea3-4716-9c3e-1815f179c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folders = [\n",
    "    \"path to folder images are stored in\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d1ba0-e8ed-4e50-87d2-83ce95127183",
   "metadata": {},
   "source": [
    "Set archive_folder as the path to the folder where images should be sent after being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c5d1d-f1b2-4a2d-907d-fdd7bd07b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_folder = \"path to the folder where images are sent after processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78ddd3-18e2-467c-beb4-fe7b0236a27e",
   "metadata": {},
   "source": [
    "This code will save two files: (1) a CSV of transcription results and (2) a tab-delimited text file of transcription results.\n",
    "\n",
    "Set output_folder as the path to the folder where the CSV and text files of transcription results should be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f6fda-6fed-48bc-89b9-0359bff6ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"path to the folder where CSV and text output is saved\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914decaf-d18a-4bbc-a16b-2adda178c13c",
   "metadata": {},
   "source": [
    "Set the file names for these two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705518c2-6c32-4075-9eca-1ebd8210258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"CSV file name\"\n",
    "txt_file_name = \"Text file name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eae171-b413-45fb-9d47-2cda6426fc63",
   "metadata": {},
   "source": [
    "Ensure google-cloud-vision, google-cloud-translate, and openai are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f3ba0-977c-4731-9405-d3ff56011162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-vision\n",
    "!pip install google-cloud-translate\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320c29c-fafe-42ec-9144-18c4debc230a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd2da9-036a-4694-8e54-0d6d1057f3e0",
   "metadata": {},
   "source": [
    "# Step 1: OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb0021-a303-4dc1-aa9a-cbd5d23c9067",
   "metadata": {},
   "source": [
    "Load the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8258e8-b85d-42b0-8a2f-7277e86a6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision \n",
    "import json\n",
    "import cv2\n",
    "import numpy as np \n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from langdetect import detect\n",
    "from google.cloud import translate_v2 as translate\n",
    "import concurrent.futures\n",
    "import re\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import io\n",
    "import concurrent.futures\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949ef7f-8ffe-468c-8751-43d4eaf4de22",
   "metadata": {},
   "source": [
    "Define the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8c545-840c-4637-ab91-34cfdaf11e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a 'detect_language' function that detects the language of a text file. This function will return 'None' if the language detection fails.\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Write a function that will remove sequences of numbers that commonly correspond to ruler text.\n",
    "def remove_specific_number_sequences(text):\n",
    "    pattern = r'0\\s1\\s2\\s3\\s4\\s5\\s6\\s7\\s8\\s9\\s10\\s*(cm)?|0\\s1\\s2\\s3\\s4\\s5\\s6\\s7\\s8\\s9\\s*(cm\\s)?10|0\\s1\\s2\\s3\\s4\\s5\\s6\\s7\\s8\\s*(cm\\s)?9\\s10|0\\s1\\s2\\s3\\s4\\s5\\s6\\s7\\s*(cm\\s)?8\\s9\\s10|0\\s1\\s2\\s3\\s4\\s5\\s6\\s7\\s*(cm\\s)?9\\s10|0\\s1\\s2\\s3\\s4\\s5\\s6\\s7\\s*(cm\\s)?8\\s9\\s10|0\\s1\\s2\\s3\\s4\\s5\\s6\\s*(cm\\s)?7\\s8\\s9\\s10|0\\s1\\s2\\s3\\s4\\s5\\s*(cm\\s)?6\\s7\\s8\\s9\\s10|0\\s1\\s2\\s3\\s4\\s*(cm\\s)?5\\s6\\s7\\s8\\s9\\s10|0\\s1\\s2\\s3\\s*(cm\\s)?4\\s5\\s6\\s7\\s8\\s9\\s10|0\\s1\\s2\\s*(cm\\s)?3\\s4\\s5\\s6\\s7\\s8\\s9\\s10|0\\s1\\s*(cm\\s)?2\\s3\\s4\\s5\\s6\\s7\\s8\\s9\\s10|0\\s*(cm\\s)?1\\s2\\s3\\s4\\s5\\s6\\s7\\s8\\s9\\s10|cm\\s0\\s1\\s2\\s3\\s4\\s5\\s6\\s7\\s8\\s9\\s10|01\\s(cm\\s)?1\\s2\\s3\\s4\\s5\\s6\\s7\\s8\\s9\\s10'\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Define a list of non-Latin languages that may occur in Asian specimens.\n",
    "asian_non_latin_languages = [\n",
    "    'am',  # Amharic\n",
    "    'ar',  # Arabic\n",
    "    'bn',  # Bengali\n",
    "    'gu',  # Gujarati\n",
    "    'iw',  # Hebrew\n",
    "    'hi',  # Hindi\n",
    "    'ja',  # Japanese\n",
    "    'kn',  # Kannada\n",
    "    'ko',  # Korean\n",
    "    'ml',  # Malayalam\n",
    "    'mr',  # Marathi\n",
    "    'ru',  # Russian\n",
    "    'ta',  # Tamil\n",
    "    'te',  # Telugu\n",
    "    'th',  # Thai\n",
    "    'zh-CN',  # Chinese (PRC)\n",
    "    'zh-TW',  # Chinese (Taiwan)\n",
    "    'ur',  # Urdu\n",
    "    'vi'  # Vietnamese\n",
    "]\n",
    "\n",
    "# Write a function to detect language and translate if it is within asian_non_latin_languages \n",
    "def translate_text(text, dest_lang='en'):\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    # Detect the language of the text\n",
    "    detected_language = translate_client.detect_language(text)['language']\n",
    "\n",
    "    # Check if the detected language is in asian_non_latin_languages\n",
    "    if detected_language in asian_non_latin_languages and detected_language != dest_lang:\n",
    "        # Translate the content to English\n",
    "        translation = translate_client.translate(\n",
    "            text,\n",
    "            target_language=dest_lang,\n",
    "        )\n",
    "        return translation['translatedText']\n",
    "    else:\n",
    "        # If the language is not in asian_non_latin_languages or is already English, do not translate\n",
    "        return text\n",
    "\n",
    "# Write a 'process_image' function\n",
    "def process_image(image_path, root):\n",
    "    \"\"\"Detects text in images stored in a local folder.\"\"\"\n",
    "\n",
    "    # Check if the image has already been processed\n",
    "    if image_path in processed_images:\n",
    "        return\n",
    "    \n",
    "    # Initialize the Google Cloud Vision client\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    try:\n",
    "        # Set the folder name\n",
    "        folder_name = os.path.basename(root)\n",
    "        \n",
    "        # Read the image file\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            image_content = image_file.read()\n",
    "\n",
    "        # Convert TIFF to JPEG with compression\n",
    "        with Image.open(io.BytesIO(image_content)) as img:\n",
    "            # Convert to RGB\n",
    "            img = img.convert('RGB')\n",
    "            \n",
    "            # Create an in-memory byte stream to hold JPEG image data\n",
    "            jpeg_image_stream = io.BytesIO()\n",
    "            \n",
    "            # Save the JPEG image to the byte stream\n",
    "            img.save(jpeg_image_stream, format='JPEG', quality=85)\n",
    "            \n",
    "            # Get JPEG image content as bytes\n",
    "            jpeg_image_content = jpeg_image_stream.getvalue()\n",
    "\n",
    "        # Create a Vision API image object\n",
    "        image = vision.Image(content=jpeg_image_content)\n",
    "\n",
    "        # Run text detection\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "\n",
    "        # Define a regular expression pattern to match \"MO-\" followed by numbers.\n",
    "        # For MO specimens, this will locate the specimen barcode.\n",
    "        pattern = r'MO-\\d+\\s'\n",
    "        \n",
    "        # Iterate through lines of the text\n",
    "        for text_annotation in texts:\n",
    "            text_annotation.description = re.sub(pattern, '', text_annotation.description)\n",
    "\n",
    "        # Create a dictionary to store the results\n",
    "        results = {\"Texts\": []}\n",
    "\n",
    "        for text_annotation in texts:\n",
    "            text_data = {\n",
    "                \"description\": text_annotation.description,\n",
    "                \"bounds\": [(vertex.x, vertex.y) for vertex in text_annotation.bounding_poly.vertices]\n",
    "            }\n",
    "            results[\"Texts\"].append(text_data)\n",
    "        \n",
    "        # Extract the currentBarcodeID from the image_path\n",
    "        currentBarcodeID = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(\n",
    "                \"{}\\nFor more info on error messages, check: \"\n",
    "                \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "            )\n",
    "\n",
    "        # Extract the first description\n",
    "        first_description = results['Texts'][0]['description']\n",
    "\n",
    "        # Split the text into segments\n",
    "        segments = first_description.split('\\n') \n",
    "        \n",
    "        # Individually translate each segment\n",
    "        translated_segments = []\n",
    "        for segment in segments:\n",
    "            translated_segment = translate_text(segment, dest_lang='en')\n",
    "            # Replace line breaks with spaces\n",
    "            translated_segment = translated_segment.replace('\\n', ' ')\n",
    "            translated_segments.append(translated_segment)\n",
    "        translated_text = translated_segments\n",
    "\n",
    "        # Create a single string from the list\n",
    "        translated_text_str = ' '.join(translated_text)\n",
    "        \n",
    "        # Again replace line breaks with spaces\n",
    "        translated_text = translated_text_str.replace('\\n', ' ')\n",
    "\n",
    "        # Remove the sequences that commonly correspond to ruler text\n",
    "        translated_text = remove_specific_number_sequences(translated_text)\n",
    "\n",
    "        # Create a list with currentBarcodeID, folder_name, and translated_text\n",
    "        image_data = [currentBarcodeID, folder_name, translated_text]\n",
    "\n",
    "        # Append the current image's data to all_image_data\n",
    "        all_image_data.append(image_data)\n",
    "\n",
    "        # Move the processed image to the archive_folder folder\n",
    "        os.makedirs(archive_folder, exist_ok=True)\n",
    "        archive_path = os.path.join(archive_folder, os.path.basename(image_path))\n",
    "        shutil.move(image_path, archive_path)\n",
    "        print(f\"Moved {image_path} to {archive_path}\")\n",
    "\n",
    "        # After processing, add the image path to the set of processed images\n",
    "        processed_images.add(image_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during image processing\n",
    "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "\n",
    "# Write a function that performs parallel image processing\n",
    "def detect_text_local_folder(input_folders):\n",
    "    \"\"\"Detects text in images stored in local folders.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    # Track the processed subfolders\n",
    "    processed_subfolders = set()\n",
    "    all_image_paths = [] \n",
    "\n",
    "    for input_folder in input_folders:\n",
    "        # Use os.walk to cycle through subdirectories and files\n",
    "        for root, dirs, files in os.walk(input_folder):\n",
    "            image_paths = []\n",
    "            \n",
    "            for file_name in files:\n",
    "                if file_name.lower().endswith(('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "                    image_path = os.path.join(root, file_name)\n",
    "                    print(f\"Processing image: {image_path} in root: {root}\")\n",
    "                    process_image(image_path, root)\n",
    "                    image_paths.append(image_path)\n",
    "            \n",
    "            # Add the current root directory to processed_subfolders if it contains images\n",
    "            if image_paths:\n",
    "                processed_subfolders.add(root)\n",
    "            \n",
    "            # Add all image paths\n",
    "            all_image_paths.extend(image_paths)\n",
    "\n",
    "    # Perform parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        for image_path in all_image_paths:\n",
    "            print(f\"Submitting image processing task for: {image_path}\")\n",
    "            executor.submit(process_image, image_path, os.path.dirname(image_path)) \n",
    "\n",
    "    # Move all empty subfolders in input_folders to the archive_folder\n",
    "    for input_folder in input_folders:\n",
    "        for root, dirs, _ in os.walk(input_folder):\n",
    "            if root not in processed_subfolders and not dirs and not os.listdir(root):\n",
    "                subfolder_to_move = root\n",
    "                archive_subfolder_path = os.path.join(archive_folder, os.path.relpath(subfolder_to_move, input_folder))\n",
    "                \n",
    "                if not os.path.exists(archive_subfolder_path):\n",
    "                    shutil.move(subfolder_to_move, archive_subfolder_path)\n",
    "                    print(f\"Moved empty subfolder {subfolder_to_move} to {archive_subfolder_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3899657-4354-4370-acce-7ab40d537c4f",
   "metadata": {},
   "source": [
    "Initialize processed_images to keep track of processed image paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8174cd0-41b7-4cb0-a17c-66e53cb1a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890de61-4132-4cfd-b739-fb5a192a0b8e",
   "metadata": {},
   "source": [
    "Initialize an empty list to collect data for each image. The column headers are set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01496388-2d5f-4326-8efd-67b20143ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_data = [[\"barcodeID\", \"storedUnder\", \"translatedText\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff7b1-0ab4-4e2f-a062-c970fe2e4803",
   "metadata": {},
   "source": [
    "Run detect_text_local_folder(input_folder). This will loop across all images within the input_folder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b94958-c773-4bed-a457-4c844a4ecc0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detect_text_local_folder(input_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2028cc61-1a5d-4ba4-8e92-964e74e2457d",
   "metadata": {},
   "source": [
    "Check to see whether all_image_data was created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79a2a0-d833-4a65-8309-9215d5d8a129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221eada-5da0-4ea2-899f-bb5a4b986a41",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b8f25-7472-4b1b-bea6-2e3e6d8cd629",
   "metadata": {},
   "source": [
    "# Step 2: ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca052dc-1852-446c-a381-f5b874030828",
   "metadata": {},
   "source": [
    "Load the following libraries and set the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832bd57-ca1e-4ee7-ac93-00750891cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set client as your unique OpenAI API key.\n",
    "client = OpenAI(\n",
    "    api_key = \"API key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424e1c8-4baa-4ccc-b48c-c2f19efd82f7",
   "metadata": {},
   "source": [
    "Save all_image_data as HerbariumSpecimenData data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32878bb8-f34f-441c-9344-44f55cb8cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HerbariumSpecimenData = pd.DataFrame(all_image_data[1:], columns=all_image_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510d8c5-1e25-43dc-9cd4-818cd9d20237",
   "metadata": {},
   "source": [
    "Create a data frame for storing the results from the following loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a8d4d-fca4-4d9a-812b-00b9df0ee0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['barcodeID', 'AccessionNumber', 'Continent', 'Country', 'LocalityDescription', \n",
    "                                'Elevation', 'CollectionDay', 'CollectionMonth', 'CollectionYear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3e414-8f01-4db1-be4c-d5e502a95387",
   "metadata": {},
   "source": [
    "Run a loop that uses OpenAI's GPT-3.5 Turbo model to extract data from specimen text for transcriptions. This loop will process all specimens in HerbariumSpecimenData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbb9b0-f9d6-414c-a620-13f8854b240f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in HerbariumSpecimenData.iterrows():\n",
    "    \n",
    "    ## Store the barcode and specimen text as objects.\n",
    "    barcode_id = row['barcodeID']\n",
    "    translated_text = row['translatedText']\n",
    "    \n",
    "    ## Accession Number\n",
    "    accessionNumber = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is label text from an herbarium specimen: '{translated_text}'. What is the accession number of this herbarium specimen? Return only the number and no extraneous text.\",\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    accessionNumberValue = accessionNumber.choices[0].message.content\n",
    "\n",
    "\n",
    "    ## Continent\n",
    "    # Set continent as either \"Asia\" or \"Africa\", depending on the project.\n",
    "    # continentValue = \"Asia\"\n",
    "    # continentValue = \"Africa\"\n",
    "\n",
    "\n",
    "    ## Country\n",
    "    country = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is label text from an herbarium specimen: '{translated_text}'. In what country was this herbarium specimen collected? Return only the country and no extraneous text.\",\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    countryValue = country.choices[0].message.content\n",
    "\n",
    "\n",
    "    ## Locality Description\n",
    "    localityDescription = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is label text from an herbarium specimen: '{translated_text}'. What is the locality description for where this herbarium specimen was collected? Return only the locality description and no extraneous text.\",\n",
    "                \"temperature\": 0.2,\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    localityDescriptionValue = localityDescription.choices[0].message.content\n",
    "    \n",
    "\n",
    "    ## Elevation\n",
    "    elevation = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is label text from an herbarium specimen: '{translated_text}'. What elevation was this herbarium specimen collected at? Return only the elevation with units and no extraneous text.\",\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    elevationValue = elevation.choices[0].message.content\n",
    "\n",
    "\n",
    "    ## Collection Day\n",
    "    collectionDay = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is label text from an herbarium specimen: '{translated_text}'. What is the day of month that this herbarium specimen was collected on? Return only the day of month as a numeric digit, with no extraneous text.\",\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    collectionDayValue = collectionDay.choices[0].message.content\n",
    "\n",
    "\n",
    "    ## Collection Month\n",
    "    collectionMonth = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is label text from an herbarium specimen: '{translated_text}'. What is the month that this herbarium specimen was collected in? Return only the month as a numeric digit from 1 to 12, with no extraneous text.\",\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    collectionMonthValue = collectionMonth.choices[0].message.content\n",
    "\n",
    "\n",
    "    ## Collection Year\n",
    "    collectionYear = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is label text from an herbarium specimen: '{translated_text}'. What is the year that this herbarium specimen was collected in? Return only the year as a four digit number, with no extraneous text.\",\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    # Extract the output\n",
    "    collectionYearValue = collectionYear.choices[0].message.content\n",
    "\n",
    "\n",
    "    ## Create a new row for the results data frame\n",
    "    new_row = pd.DataFrame({'barcodeID': [barcode_id], 'AccessionNumber': [accessionNumberValue], 'Continent': [continentValue], 'Country': [countryValue], \n",
    "                            'LocalityDescription': [localityDescriptionValue], 'Elevation': [elevationValue], 'CollectionDay': [collectionDayValue], \n",
    "                            'CollectionMonth': [collectionMonthValue], 'CollectionYear': [collectionYearValue]})\n",
    "    \n",
    "    # Append the new row to the results data frame\n",
    "    results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "    # Print the row that was processed\n",
    "    print(f\"{index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0baf1-9bc9-49ae-a57c-0e59e51b78e2",
   "metadata": {},
   "source": [
    "Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d235dc-d671-467d-9bcf-8023990331ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12a54a-f70c-4ea7-9610-2672d38a5ffb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962b886-cc6f-4b2a-901e-cfdf572810ae",
   "metadata": {},
   "source": [
    "# Step 3: Formatting for Tropicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ae669-64b0-4ef8-af4b-cd505ee1e018",
   "metadata": {},
   "source": [
    "This section formats the data in the results data frame for upload to Tropicos.org, the Missouri Botancial Garden's database for storing specimen records. Accordingly, this section is tailored specifically for Tropicos upload, and would need to be modified for use with other databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9664f-016c-44e4-9668-039c684e7c20",
   "metadata": {},
   "source": [
    "Load the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0c1bf-96cf-4dc8-8237-94ec3e95209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262aa59-58c0-4c90-9ef7-d308d5c55be1",
   "metadata": {},
   "source": [
    "Save results as GPTsheetsOutput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384da03-b851-40a8-ab4f-b22bfd8537e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTsheetsOutput = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223701d-cfda-478f-99b3-89151bdac527",
   "metadata": {},
   "source": [
    "Initialize an empty data frame that contains all the columns required for bulk specimen upload to Tropicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b42169-1c30-4777-990c-813f9d20cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "specimenBulkData = pd.DataFrame(columns=[\"Authority\", \"AuthorityKey\", \"CollectorString\", \"SeniorCollectorPersonID\", \"CollectionNumber\", \"CollectionEventID\", \n",
    "                                         \"EventNumber\", \"DeterminationQualifier\", \"DeterminedBy\", \"DeterminedByPersonID\", \"DeterminationDay\", \"DeterminationMonth\", \n",
    "                                         \"DeterminationYear\", \"DeterminationInstitution\", \"LocationID\", \"MinimumLatitude\", \"MinimumLongitude\", \"MaximumLatitude\", \n",
    "                                         \"MaximumLongitude\", \"CoordinateMethod\", \"MinimumElevation\", \"MaximumElevation\", \"ElevationUnit\", \"ElevationMethod\", \n",
    "                                         \"UncertaintyMeters\", \"Sources\", \"CoordinateNote\", \"MinimumDay\", \"MinimumMonth\", \"MinimumYear\", \"MaximumDay\", \"MaximumMonth\", \n",
    "                                         \"MaximumYear\", \"LocalityNote\", \"DescriptionNote\", \"HabitatNote\", \"VegetationDescription\", \"GeneralNote\", \"Duplicates\", \n",
    "                                         \"IsCultivated\", \"FooterID\", \"Institutions\", \"DateLanguage\", \"OtherCollectorIDs\", \"GeographicKeywords\", \"GeolocationNote\", \n",
    "                                         \"ConditionEcologicalValues\", \"TownshipRangeNote\", \"Barcodes\", \"GeneralKeywords\", \"PreviousDeterminationIDs\", \n",
    "                                         \"ExsiccataePublicationID\", \"ExsiccataeNumber\", \"ScanningProjectCode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd5763-fbe3-49c0-8e03-539923804e86",
   "metadata": {},
   "source": [
    "In Tropicos, locations are assigned unique identifiers. These identifiers must be included in specimen data that is bulk uploaded to Tropicos. In preparation for fuzzy matching of ChatGPT output to Tropicos identifers, load the Tropicos location list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfd66a-bc31-4a0c-a182-7c946ff4b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "LocationsDataDumpPath = \"path to file with list of location identifiers\"\n",
    "LocationsDataDump = pd.read_csv(LocationsDataDumpPath, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97627606-5193-4398-a55d-65afbca37d13",
   "metadata": {},
   "source": [
    "Define the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8147d1ac-d59d-4f88-88d6-1a1298970e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 'find_matching_id' function for fuzzy matching\n",
    "def find_matching_id(ocr_output, tropicos_data, id_column):\n",
    "    # Find the string that best matches the OCR output\n",
    "    match = process.extractOne(ocr_output, tropicos_data, scorer=fuzz.token_sort_ratio)\n",
    "    \n",
    "    # Extract the matched string and its index\n",
    "    matched_string, score, index = match\n",
    "    \n",
    "    # Use the index to find the corresponding identifier\n",
    "    matched_id = id_column.iloc[index]\n",
    "    \n",
    "    return matched_id\n",
    "\n",
    "# Define a 'has_numbers' function that returns TRUE if a string contains any numbers and FALSE if it does not\n",
    "def has_numbers(inputString):\n",
    "    inputString = str(inputString)\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "# Define a 'contains_only_digits' function that returns TRUE if a string contains only digits and FALSE if it contains any characters that are not digits\n",
    "def contains_only_digits(input_string):\n",
    "    for char in input_string:\n",
    "        if not char.isdigit():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Define an 'extract_elevation_info' function that extracts numeric elevation values and units, including their ranges\n",
    "def extract_elevation_info(elevation_str):\n",
    "    patternElevation = r'(\\d+(\\.\\d+)?)-?(\\d+(\\.\\d+)?)?\\s*([mf]?)'\n",
    "    matchElevation = re.search(patternElevation, elevation_str, re.I)\n",
    "    \n",
    "    if matchElevation:\n",
    "        min_value_str, _, max_value_str, _, unit_str = matchElevation.groups()\n",
    "        \n",
    "        min_value = float(min_value_str) if min_value_str else None\n",
    "        max_value = float(max_value_str) if max_value_str else None\n",
    "\n",
    "        # Ensure the values are within the specified range\n",
    "        if min_value is not None and (min_value < -2000 or min_value > 30000):\n",
    "            min_value = None\n",
    "        if max_value is not None and (max_value < -2000 or max_value > 30000):\n",
    "            max_value = None\n",
    "\n",
    "        # Check if unit is present\n",
    "        unit = unit_str.lower() if unit_str else ''\n",
    "\n",
    "        # Convert to integers only if they are valid numbers\n",
    "        if min_value is not None:\n",
    "            min_value = int(min_value)\n",
    "        if max_value is not None:\n",
    "            max_value = int(max_value)\n",
    "\n",
    "        # Return the results with empty strings if values are None\n",
    "        if min_value is not None and max_value is not None:\n",
    "            return min_value, max_value, unit\n",
    "        elif min_value is not None:\n",
    "            return min_value, '', unit\n",
    "        elif max_value is not None:\n",
    "            return '', max_value, unit\n",
    "    \n",
    "    return '', '', ''\n",
    "\n",
    "# Define a 'combine_location_fields' function that creates a string of continent and country from the location data dump\n",
    "def combine_location_fields(row):\n",
    "    fields = [row['ContinentName'], row['CountryName']]\n",
    "    non_nan_fields = [str(field) for field in fields if not pd.isna(field)]\n",
    "    return ' '.join(non_nan_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec09c4-18a2-49e5-8f42-139c45942acf",
   "metadata": {},
   "source": [
    "Clean the location data in GPTsheetsOutput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272e692-6b63-4870-bda2-2c90efa42cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "GPTsheetsOutput.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Replace 'USA' with 'United States' in the 'Continent' column\n",
    "GPTsheetsOutput['Country'] = GPTsheetsOutput['Country'].replace('USA', 'United States')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736d058-4224-4b53-be37-2cf5376491e5",
   "metadata": {},
   "source": [
    "Perform general cleaning on the entire GPTsheetsOutput spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde07de-3a2a-474d-ba3f-75a5944b4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 'clean_cell' function that applies cleaning rules to a single cell\n",
    "def clean_cell(cell):\n",
    "    if isinstance(cell, str):\n",
    "        original_cell = cell \n",
    "        cell = cell.lower()\n",
    "        if cell in ['#ref!', '#error!']:\n",
    "            return ''\n",
    "        if any(keyword in cell for keyword in [' no ', ' not ', 'no ', 'not ', 'unknown', 'sorry', 'apologies', 'apologize', 'unable']):\n",
    "            return ''\n",
    "        return original_cell \n",
    "    return cell\n",
    "\n",
    "# Apply the 'clean_cell' function to each cell\n",
    "GPTsheetsOutput = GPTsheetsOutput.map(clean_cell)\n",
    "\n",
    "# Replace missing values with ''\n",
    "GPTsheetsOutput = GPTsheetsOutput.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca49a35-377b-425f-b786-1ee17311f648",
   "metadata": {},
   "source": [
    "Run a loop that uses data in GPTsheetsOutput to add a row per specimen to specimenBulkData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac7459-fb63-4793-bc5c-0e62924240ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store the rows of data\n",
    "rows_to_append = []\n",
    "\n",
    "for currentRow in range(len(GPTsheetsOutput)):\n",
    "\n",
    "    # Project code\n",
    "    # Set the current project code as either \"ASIA\" or \"AFRICA\".\n",
    "    # currentProjectCode = \"ASIA\"\n",
    "    # currentProjectCode = \"AFRICA\"\n",
    "    \n",
    "    # Collector\n",
    "    # Set collector as a generic collector for all specimens.\n",
    "    currentCollectorID = '100152366'\n",
    "    currentCollectorNameTropicos = 'AI_COLLECTOR, *'\n",
    "    \n",
    "\n",
    "    # Location fuzzy matching\n",
    "    # Only set location to country. If country can't be found, set to continent.\n",
    "\n",
    "    # Continent\n",
    "    currentContinentNameOCR = str(GPTsheetsOutput.loc[currentRow, 'Continent'])\n",
    "\n",
    "    if (currentContinentNameOCR == ''):\n",
    "        currentContinentID = ''\n",
    "        currentContinentNameTropicos = ''\n",
    "        fuzzyScoreContinentName = 0\n",
    "    else:\n",
    "        # Use fuzzy matching to find the continent and its Tropicos ID\n",
    "        currentContinentID = find_matching_id(currentContinentNameOCR, LocationsDataDump['ContinentName'], LocationsDataDump['ContinentID'])\n",
    "    \n",
    "        # Use the currentContinentID to find the current continent name\n",
    "        # Check if there are any matching rows\n",
    "        matching_rows_continent = LocationsDataDump[LocationsDataDump['ContinentID'] == currentContinentID]\n",
    "\n",
    "        if not matching_rows_continent.empty:\n",
    "            currentContinentNameTropicos = matching_rows_continent['ContinentName'].values[0]\n",
    "            # Obtain 'fuzzy matching score' for continent (100 is perfect match, 0 is no match)\n",
    "            fuzzyScoreContinentName = fuzz.token_set_ratio(currentContinentNameOCR, currentContinentNameTropicos)\n",
    "        else:\n",
    "            # Handle the case where there are no matching rows\n",
    "            currentContinentNameTropicos = ''\n",
    "            fuzzyScoreContinentName = 0\n",
    "        \n",
    "    \n",
    "    # Country\n",
    "    currentCountryNameOCR = str(GPTsheetsOutput.loc[currentRow, 'Country'])\n",
    "\n",
    "    if (currentCountryNameOCR == ''):\n",
    "        currentCountryID = ''\n",
    "        currentCountryNameTropicos = ''\n",
    "        fuzzyScoreCountryName = 0\n",
    "    else:\n",
    "        # Use fuzzy matching to find the country and its Tropicos ID\n",
    "        currentCountryID = find_matching_id(currentCountryNameOCR, LocationsDataDump['CountryName'], LocationsDataDump['CountryID'])\n",
    "    \n",
    "        # Use the currentCountryID to find the current country name\n",
    "        # Check if there are any matching rows\n",
    "        matching_rows_country = LocationsDataDump[LocationsDataDump['CountryID'] == currentCountryID]\n",
    "\n",
    "        if not matching_rows_country.empty:\n",
    "            currentCountryNameTropicos = matching_rows_country['CountryName'].values[0]\n",
    "            # Obtain 'fuzzy matching score' for country (100 is perfect match, 0 is no match)\n",
    "            fuzzyScoreCountryName = fuzz.token_set_ratio(currentCountryNameOCR, currentCountryNameTropicos)\n",
    "        else:\n",
    "            # Handle the case where there are no matching rows\n",
    "            currentCountryNameTropicos = ''\n",
    "            fuzzyScoreCountryName = 0\n",
    "\n",
    "\n",
    "    # Use the currentContinentID and currentCountryID to find the location ID.              \n",
    "    filter_conditions = (\n",
    "        ((LocationsDataDump['ContinentID'] == currentContinentID) & (fuzzyScoreContinentName > 75)) &\n",
    "        ((LocationsDataDump['CountryID'] == currentCountryID) & (fuzzyScoreCountryName > 75))\n",
    "    )   \n",
    "    if filter_conditions.any():\n",
    "        currentLocationID = LocationsDataDump.loc[filter_conditions, 'LocationID'].values[0]\n",
    "    else:\n",
    "        # If no result is found, try continent\n",
    "        filter_conditions = (\n",
    "            ((LocationsDataDump['ContinentID'] == currentContinentID) & (fuzzyScoreContinentName > 75))\n",
    "        )  \n",
    "        if filter_conditions.any():\n",
    "            currentLocationID = LocationsDataDump.loc[filter_conditions, 'LocationID'].values[0]\n",
    "        else:\n",
    "            # If no result is found, return an emtpy string\n",
    "            currentLocationID = \"\"\n",
    "            \n",
    "\n",
    "    # Accession number\n",
    "    # If the accession number is 6 or more digits in length, set accession number as the verbatim accession number,\n",
    "    # if not, set accession number as blank.\n",
    "    if (len(GPTsheetsOutput.loc[currentRow, 'AccessionNumber']) >= 6):\n",
    "        currentAccessionNumber = GPTsheetsOutput.loc[currentRow, 'AccessionNumber']\n",
    "    else:\n",
    "        currentAccessionNumber = \"\"\n",
    "\n",
    "    # If the accession number contains any characters aside from numeric digits, set it to an empty string\n",
    "    if (contains_only_digits(currentAccessionNumber) == False):\n",
    "        currentAccessionNumber = ''\n",
    "\n",
    "\n",
    "    # Collection number\n",
    "    # Set the collection number for every specimen to 's.n.'. This will be updated during manual editing.\n",
    "    currentCollectionNumber = 's.n.'\n",
    "    \n",
    "\n",
    "    # For the barcode field in Tropicos, create a list that contains the following, each separated by a comma and surrounded by parentheses:\n",
    "    # institution, barcode, accession number, barcode object (e.g., (MO,MO-1859342,3794786,sheet)\n",
    "    currentBarcodeID = GPTsheetsOutput.loc[currentRow, 'barcodeID']\n",
    "    currentBarcodeList = f\"(MO,{currentBarcodeID},{currentAccessionNumber},sheet)\"\n",
    "    \n",
    "\n",
    "    # Create objects for the day, month, and year of collection.\n",
    "    # Return no value for invalid dates and remove non-numeric text\n",
    "\n",
    "    # Collection year:\n",
    "    # If CollectionYear is 4 digits, return this as currentCollectionYear\n",
    "    if (len(str(GPTsheetsOutput.loc[currentRow, 'CollectionYear'])) == 4):\n",
    "        currentCollectionYear = GPTsheetsOutput.loc[currentRow, 'CollectionYear']\n",
    "    elif (has_numbers(GPTsheetsOutput.loc[currentRow, 'CollectionYear']) == True):\n",
    "        pattern = r'is (\\d+)' \n",
    "        match = re.search(pattern, str(GPTsheetsOutput.loc[currentRow, 'CollectionYear']))\n",
    "        if match:\n",
    "            currentCollectionYear = match.group(1)\n",
    "        else:\n",
    "            currentCollectionYear = \"\"\n",
    "    else:\n",
    "        currentCollectionYear = \"\" \n",
    "\n",
    "    # If currentCollectionYear is not an integer (i.e., contains 4 characters, but not all are digits) set currentCollectionYear as an empty string\n",
    "    if currentCollectionYear.isdigit():\n",
    "        currentCollectionYear = int(currentCollectionYear)\n",
    "    else:\n",
    "        currentCollectionYear = ''\n",
    "    \n",
    "    # If currentCollectionYear is 0, make currentCollectionYear have no value\n",
    "    if currentCollectionYear != '': \n",
    "        if currentCollectionYear == 0:\n",
    "            currentCollectionYear = \"\"\n",
    "    else:\n",
    "        currentCollectionYear = \"\"\n",
    "\n",
    "    # Check if currentCollectionYear contains non-numeric characters, and if so, set it to an empty string\n",
    "    if (currentCollectionYear == ''):\n",
    "        currentCollectionYear = ''\n",
    "    elif not re.match(r'^\\d+$', str(currentCollectionYear)):\n",
    "        currentCollectionYear = ''\n",
    "    else:\n",
    "        currentCollectionYear = currentCollectionYear\n",
    "    \n",
    "    # If currentCollectionYear is not within 1850-2030, set currentCollectionYear to empty string\n",
    "    if (currentCollectionYear == ''):\n",
    "        currentCollectionYear = ''\n",
    "    elif ((1850 <= int(currentCollectionYear) <= 2030) == False):\n",
    "        currentCollectionYear = ''\n",
    "    else:\n",
    "        currentCollectionYear = currentCollectionYear\n",
    "    \n",
    "    \n",
    "    # Collection month:\n",
    "    # If CollectionMonth is less than 3 digits, return this as currentCollectionMonth\n",
    "    if (len(str(GPTsheetsOutput.loc[currentRow, 'CollectionMonth'])) < 3): \n",
    "        currentCollectionMonth = GPTsheetsOutput.loc[currentRow, 'CollectionMonth']\n",
    "    elif (has_numbers(GPTsheetsOutput.loc[currentRow, 'CollectionMonth']) == True):\n",
    "        pattern = r'is (\\d+)'\n",
    "        match = re.search(pattern, str(GPTsheetsOutput.loc[currentRow, 'CollectionMonth']))\n",
    "        if match:\n",
    "            currentCollectionMonth = match.group(1)\n",
    "            if currentCollectionMonth.isdigit():\n",
    "                currentCollectionMonth = int(currentCollectionMonth)\n",
    "            else:\n",
    "                currentCollectionMonth = \"\" \n",
    "        else:\n",
    "            currentCollectionMonth = \"\"  \n",
    "    else:\n",
    "        currentCollectionMonth = \"\"\n",
    "\n",
    "    if currentCollectionMonth != '':\n",
    "        if isinstance(currentCollectionMonth, str) and currentCollectionMonth.isdigit():\n",
    "            currentCollectionMonth = int(currentCollectionMonth)\n",
    "        else:\n",
    "            currentCollectionMonth = \"\" \n",
    "        if currentCollectionMonth == 0:\n",
    "            currentCollectionMonth = \"\"\n",
    "    else:\n",
    "        currentCollectionMonth = \"\"\n",
    "\n",
    "    # Check if currentCollectionMonth contains non-numeric characters, and if so, set it to an empty string\n",
    "    if (currentCollectionMonth == ''):\n",
    "        currentCollectionMonth = ''\n",
    "    elif not re.match(r'^\\d+$', str(currentCollectionMonth)):\n",
    "        currentCollectionMonth = ''\n",
    "    else:\n",
    "        currentCollectionMonth = currentCollectionMonth\n",
    "    \n",
    "    # If currentCollectionMonth is not within 1-12, set currentCollectionMonth to empty string\n",
    "    if (currentCollectionMonth == ''):\n",
    "        currentCollectionMonth = ''\n",
    "    elif ((1 <= int(currentCollectionMonth) <= 12) == False):\n",
    "        currentCollectionMonth = ''\n",
    "    else:\n",
    "        currentCollectionMonth = currentCollectionMonth\n",
    "        \n",
    "    \n",
    "    # Collection day:\n",
    "    # If there is no currentCollectionMonth, return no value for currentCollectionDay\n",
    "    if (has_numbers(currentCollectionMonth) == False): \n",
    "        currentCollectionDay = \"\"\n",
    "    # If CollectionDay is less than 3 digits, return this as currentCollectionDay\n",
    "    elif (len(str(GPTsheetsOutput.loc[currentRow, 'CollectionDay'])) < 3):\n",
    "        currentCollectionDay = GPTsheetsOutput.loc[currentRow, 'CollectionDay']\n",
    "    elif (has_numbers(GPTsheetsOutput.loc[currentRow, 'CollectionDay']) == True):\n",
    "        pattern = r'is (\\d+)' \n",
    "        match = re.search(pattern, str(GPTsheetsOutput.loc[currentRow, 'CollectionDay']))\n",
    "        if match:\n",
    "            currentCollectionDay = match.group(1)\n",
    "            if currentCollectionDay.isdigit():\n",
    "                currentCollectionDay = int(currentCollectionDay)\n",
    "            else:\n",
    "                currentCollectionDay = \"\"  \n",
    "        else:\n",
    "            currentCollectionDay = \"\"  \n",
    "    else:\n",
    "        currentCollectionDay = \"\"\n",
    "\n",
    "    # Check if currentCollectionDay contains non-numeric characters, and if so, set it to an empty string\n",
    "    if (currentCollectionDay == ''):\n",
    "        currentCollectionDay = ''\n",
    "    elif not re.match(r'^\\d+$', str(currentCollectionDay)):\n",
    "        currentCollectionDay = ''\n",
    "    else:\n",
    "        currentCollectionDay = currentCollectionDay\n",
    "\n",
    "    # If currentCollectionDay is not within 1-31, set currentCollectionDay to empty string\n",
    "    if (currentCollectionDay == ''):\n",
    "        currentCollectionDay = ''\n",
    "    elif ((1 <= int(currentCollectionDay) <= 31) == False):\n",
    "        currentCollectionDay = ''\n",
    "    else:\n",
    "        currentCollectionDay = currentCollectionDay\n",
    "\n",
    "\n",
    "    # Create objects for elevation and the unit of elevation\n",
    "    currentElevationInfo = extract_elevation_info(GPTsheetsOutput.loc[currentRow, 'Elevation'])\n",
    "    currentMinimumElevation = currentElevationInfo[0]\n",
    "    currentMaximumElevation = currentElevationInfo[1]\n",
    "    currentElevationUnit = currentElevationInfo[2]\n",
    "\n",
    "    # If currentMinimumElevation is not within -2000-6000, set currentMinimumElevation to empty string\n",
    "    if (currentMinimumElevation == ''):\n",
    "        currentMinimumElevation = ''\n",
    "    elif ((currentElevationUnit == 'm') and (-2000 <= int(currentMinimumElevation) <= 6000) == False):\n",
    "        currentMinimumElevation = ''\n",
    "    else:\n",
    "        currentMinimumElevation = currentMinimumElevation\n",
    "\n",
    "    # If currentMaximumElevation is not within -2000-6000, set currentMaximumElevation to empty string\n",
    "    if (currentMaximumElevation == ''):\n",
    "        currentMaximumElevation = ''\n",
    "    elif ((currentElevationUnit == 'm') and (-2000 <= int(currentMaximumElevation) <= 6000) == False):\n",
    "        currentMaximumElevation = ''\n",
    "    else:\n",
    "        currentMaximumElevation = currentMaximumElevation\n",
    "\n",
    "    \n",
    "    # Create object for LocalityNote\n",
    "    # Use the verbatim text from the GPT-Sheets output for these fields, unless an error occurred or no note is given.\n",
    "    currentLocalityNote = GPTsheetsOutput.loc[currentRow, 'LocalityDescription']\n",
    "\n",
    "    # Set the currentAuthority to 'missouri botanical garden'\n",
    "    currentAuthority = 'missouri botanical garden'\n",
    "\n",
    "    # Set currentAuthorityKey to 'OCR' followed by a unique number, all followed by the project code and today's date\n",
    "    today_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    currentAuthorityKey = 'OCR' + str(currentRow) + \"_\" + currentProjectCode + \"_\" + today_date + \"a\"\n",
    "\n",
    "    \n",
    "    # Create a new row to add to specimenBulkData\n",
    "    newRow = {\n",
    "        \"Authority\": currentAuthority, \n",
    "        \"AuthorityKey\": currentAuthorityKey, \n",
    "        \"CollectorString\": currentCollectorNameTropicos, \n",
    "        \"SeniorCollectorPersonID\": currentCollectorID, \n",
    "        \"CollectionNumber\": currentCollectionNumber, \n",
    "        \"CollectionEventID\": \"\",                        \n",
    "        \"EventNumber\": \"\", \n",
    "        \"DeterminationQualifier\": \"\", \n",
    "        \"DeterminedBy\": \"\", \n",
    "        \"DeterminedByPersonID\": \"\",\n",
    "        \"DeterminationDay\": \"\", \n",
    "        \"DeterminationMonth\": \"\", \n",
    "        \"DeterminationYear\": \"\", \n",
    "        \"DeterminationInstitution\": \"\", \n",
    "        \"LocationID\": currentLocationID, \n",
    "        \"MinimumLatitude\": \"\", \n",
    "        \"MinimumLongitude\": \"\", \n",
    "        \"MaximumLatitude\": \"\", \n",
    "        \"MaximumLongitude\": \"\", \n",
    "        \"CoordinateMethod\": \"\", \n",
    "        \"MinimumElevation\": currentMinimumElevation, \n",
    "        \"MaximumElevation\": currentMaximumElevation, \n",
    "        \"ElevationUnit\": currentElevationUnit, \n",
    "        \"ElevationMethod\": \"\", \n",
    "        \"UncertaintyMeters\": \"\", \n",
    "        \"Sources\": \"\", \n",
    "        \"CoordinateNote\": \"\", \n",
    "        \"MinimumDay\": currentCollectionDay, \n",
    "        \"MinimumMonth\": currentCollectionMonth, \n",
    "        \"MinimumYear\": currentCollectionYear, \n",
    "        \"MaximumDay\": \"\", \n",
    "        \"MaximumMonth\": \"\", \n",
    "        \"MaximumYear\": \"\", \n",
    "        \"LocalityNote\": currentLocalityNote, \n",
    "        \"DescriptionNote\": \"\", \n",
    "        \"HabitatNote\": \"\", \n",
    "        \"VegetationDescription\": \"\", \n",
    "        \"GeneralNote\": \"Tropicos transcription obtained using Google Cloud OCR and GPT 3.5 Turbo, followed by human editing.\", \n",
    "        \"Duplicates\": \"\", \n",
    "        \"IsCultivated\": \"\", \n",
    "        \"FooterID\": \"\", \n",
    "        \"Institutions\": \"MO\", \n",
    "        \"DateLanguage\": \"\", \n",
    "        \"OtherCollectorIDs\": \"\",\n",
    "        \"GeographicKeywords\": \"\", \n",
    "        \"GeolocationNote\": \"\", \n",
    "        \"ConditionEcologicalValues\": \"\", \n",
    "        \"TownshipRangeNote\": \"\", \n",
    "        \"Barcodes\": currentBarcodeList, \n",
    "        \"GeneralKeywords\": \"\", \n",
    "        \"PreviousDeterminationIDs\": \"\", \n",
    "        \"ExsiccataePublicationID\": \"\", \n",
    "        \"ExsiccataeNumber\": \"\",\n",
    "        \"ScanningProjectCode\": currentProjectCode,\n",
    "    }\n",
    "\n",
    "    # Append the `newRow`\n",
    "    rows_to_append.append(newRow)\n",
    "\n",
    "    # Print currentRow\n",
    "    print({currentRow})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278f1ed-56fe-4312-9bb1-d07bc0f3c092",
   "metadata": {},
   "source": [
    "Combine data frames outside the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85027d8-5810-4fd9-a8a0-54e0ceacd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each dictionary to a data frame\n",
    "dataframes_to_concat = [pd.DataFrame([newRow]) for newRow in rows_to_append]\n",
    "\n",
    "# Concatenate all the data frames outside the loop\n",
    "specimenBulkData = pd.concat([specimenBulkData] + dataframes_to_concat, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fa256-f28a-454e-9a0a-9b5137989ea3",
   "metadata": {},
   "source": [
    "Replace 'NaN' with '' in specimenBulkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27abbb-4063-4dd8-89c3-44dd2658da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "specimenBulkData = specimenBulkData.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65deec7-7d69-41ba-a6cf-6fa794f6b3f7",
   "metadata": {},
   "source": [
    "Look at specimenBulkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5b417-d309-4bb7-95c9-c6562115054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "specimenBulkData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085336fe-397f-4b16-981a-1b5c769d051f",
   "metadata": {},
   "source": [
    "Remove text from cells with non-English characters and replace line breaks with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a1303-e7e4-4fc9-a5ab-23897cb08802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cell(cell):\n",
    "    if isinstance(cell, str):\n",
    "        # Replace line breaks with spaces\n",
    "        cell = cell.replace('\\n', ' ')\n",
    "        # Replace non-English characters with an empty string\n",
    "        if re.search(r'[^\\x00-\\x7F]', cell):\n",
    "            return ''\n",
    "    return cell\n",
    "\n",
    "specimenBulkData = specimenBulkData.applymap(clean_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b5d48-16b8-4cca-8db9-0df6f5e88e78",
   "metadata": {},
   "source": [
    "Write specimenBulkData to a CSV for bulk upload to Tropicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf3514-d760-4f50-90b7-6bb53cab4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "specimenBulkData.to_csv(f'{output_folder}/{csv_file_name}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561fe3b-77ed-48ac-9a4f-c033644b3f89",
   "metadata": {},
   "source": [
    "Write specimenBulkData to a tab delimited text file for bulk upload to Tropicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e27676-f57a-4761-a215-179c452e5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "specimenBulkData.to_csv(f'{output_folder}/{txt_file_name}', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
